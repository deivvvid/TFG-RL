<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Project Directory Index</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #f8f9fa;
            padding: 40px;
        }
        h1, h2 {
            color: #333;
        }
        ul {
            list-style: none;
            padding-left: 0;
        }
        li {
            margin: 6px 0;
        }
        a {
            text-decoration: none;
            color: #0077cc;
        }
        a:hover {
            text-decoration: underline;
        }
        section {
            margin-bottom: 40px;
        }
    </style>
</head>
<body>

    <h1>Project Directory Index</h1>

    <section>
        <h2>Tabular Methods</h2>
        <ul>
            <li><a href="FrozenLake_TabularMethods.ipynb">FrozenLake_TabularMethods.ipynb</a></li>
        </ul>
    </section>

    <section>
        <h2>Approximate methods</h2>
        <ul>
            <li><a href="CartPole_DQN.ipynb">CartPole_DQN.ipynb</a></li>
            <li><a href="gym-dqn-load.py">gym-dqn-load.py</a></li>
            <li><a href="best_dqn_cartpole.pth">best_dqn_cartpole.pth</a></li>
        </ul>
    </section>

    <section>
        <h2>Single-Agent RL (SARL) Image based DQN</h2>
        <ul>
            <li><a href="Kaggle_SARL_DQN.ipynb">Kaggle_SARL_DQN.ipynb</a></li>
            <li><a href="video_SARL_DQN.mp4">video_SARL_DQN.mp4</a></li>
            <li><a href="best_dqn_model_8M.pth">best_dqn_model_8M.pth</a></li>
        </ul>
    </section>

    <section>
        <h2>Multi-Agent Reinforcement Learning (MARL)</h2>
        <ul>
            <li><a href="MARL.ipynb">MARL.ipynb</a></li>
            <li><a href="video_MARL_DQN_REINFORCE.mp4">video_MARL_DQN_REINFORCE.mp4</a></li>
        </ul>
    </section>

    <section>
        <h2>Informs delivered</h2>
        <ul>
        <li>
            <a href="informe_inicial_davidFuentesInsa.pdf">informe_inicial_davidFuentesInsa.pdf</a><br>
            <em>Initial plan:</em> Defined objectives, project scope, methodology (Kanban), early RL concepts (MDP, tabular vs. non-tabular methods). Planning focused on self-play and building from scratch.
        </li>
        <li>
            <a href="primer_informe_davidFuentesInsa.pdf">primer_informe_davidFuentesInsa.pdf</a><br>
            <em>Expanded theory:</em> Reinforced foundations of RL (agents, states, policies, returns). Detailed planning phases with Kanban and ClickUp. Introduced and structured research on DP, Monte Carlo, SARSA, and Q-learning.
        </li>
        <li>
            <a href="segon_informe_davidFuentesInsa.pdf">segon_informe_davidFuentesInsa.pdf</a><br>
            <em>Implementation stage:</em> Implemented and tested Monte Carlo, SARSA, Q-learning on FrozenLake. Added DQN on CartPole. Introduced image-based DQN with preprocessing (stacking, skipping). Initial training results included.
        </li>
        <li>
            <a href="propostaInformeFinal_davidFuentesInsa.pdf">propostaInformeFinal_davidFuentesInsa.pdf</a><br>
            <em>Near-final report:</em> Consolidated all phases including image-based DQN for Pong and multi-agent training (DQN vs. REINFORCE). Expanded results, model evaluations, added MARL complexity and PettingZoo usage. Near-final conclusions included.
        </li>
        <li>
            <a href="informeFinal_davidFuentesInsa.pdf">informeFinal_davidFuentesInsa.pdf</a><br>
            <em>Final report:</em> Complete consolidation of the project. Includes full implementations of tabular and approximate RL methods (Monte Carlo, SARSA, Q-learning, DQN), an image-based DQN trained on Atari Pong with 8M frames, and a multi-agent setup (DQN vs. REINFORCE) using PettingZoo. Detailed evaluations, architectural explanations, training strategies, and test results are provided, along with final conclusions and acknowledgements.
        </li>
    </ul>
    </section>
</body>
</html>
